{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 18:10:36.012204: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 18:10:36.030948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-01 18:10:36.044543: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-01 18:10:36.048029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 18:10:36.059425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-01 18:10:36.922673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740845451.393091   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.427436   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.428475   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.430364   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.431376   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.432341   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.523052   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.524183   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1740845451.525146   56207 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-01 18:10:51.526069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2163 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown words: 19019\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghaidaa/anaconda3/envs/tf_env_v2/lib/python3.9/site-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'enhanced_text_generator', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "2025-03-01 18:11:13.810860: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6517 - loss: 2.3847\n",
      "Epoch 1: val_loss improved from inf to 3.61218, saving model to enhanced_text_generator.keras\n",
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1392s\u001b[0m 79ms/step - accuracy: 0.6517 - loss: 2.3846 - val_accuracy: 0.6073 - val_loss: 3.6122\n",
      "Epoch 2/4\n",
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8685 - loss: 0.8159\n",
      "Epoch 2: val_loss improved from 3.61218 to 3.46729, saving model to enhanced_text_generator.keras\n",
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1398s\u001b[0m 79ms/step - accuracy: 0.8685 - loss: 0.8159 - val_accuracy: 0.6288 - val_loss: 3.4673\n",
      "Epoch 3/4\n",
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8931 - loss: 0.5876\n",
      "Epoch 3: val_loss improved from 3.46729 to 3.45921, saving model to enhanced_text_generator.keras\n",
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1444s\u001b[0m 82ms/step - accuracy: 0.8931 - loss: 0.5876 - val_accuracy: 0.6355 - val_loss: 3.4592\n",
      "Epoch 4/4\n",
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9034 - loss: 0.5031\n",
      "Epoch 4: val_loss did not improve from 3.45921\n",
      "\u001b[1m17644/17644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1442s\u001b[0m 82ms/step - accuracy: 0.9034 - loss: 0.5031 - val_accuracy: 0.6402 - val_loss: 3.4705\n"
     ]
    }
   ],
   "source": [
    "with open('anna.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "words = text.split()\n",
    "word_counts = Counter(words)\n",
    "\n",
    "vocab = ['<UNK>'] + list(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "int_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# SEQUENCE_LENGTH = 64\n",
    "SEQUENCE_LENGTH = 32\n",
    "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]\n",
    "\n",
    "split_idx = int(0.8 * len(samples))\n",
    "train_samples = samples[:split_idx]\n",
    "val_samples = samples[split_idx:]\n",
    "\n",
    "def prepare_datasets(samples):\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "    for sample in samples:\n",
    "        input_seq = [word_to_int.get(word, 0) for word in sample[:-1]]\n",
    "        target_seq = [word_to_int.get(word, 0) for word in sample[1:]]\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "    return tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = prepare_datasets(train_samples).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = prepare_datasets(val_samples).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "def load_pretrained_embeddings(embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for i, word in enumerate(vocab):\n",
    "        if i == 0:  # Leave <UNK> as zeros\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            unknown_words.append(word)\n",
    "    \n",
    "    print(f\"Number of unknown words: {len(unknown_words)}\")\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = load_pretrained_embeddings()\n",
    "\n",
    "class EnhancedTextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            # trainable=True\n",
    "            trainable=False\n",
    "        )\n",
    "        self.lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.2)\n",
    "        )\n",
    "        self.lstm2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.2)\n",
    "        )\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm1(x)\n",
    "        # x = self.lstm2(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_size = 128  \n",
    "learning_rate = 0.001\n",
    "epochs = 4\n",
    "\n",
    "model = EnhancedTextGenerator(vocab_size, embedding_dim, hidden_size)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate,\n",
    "    clipnorm=1.0  \n",
    ")\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'enhanced_text_generator.keras',  \n",
    "    monitor='val_loss',\n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    mode='min',  \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "def generate_text(model, start_string, num_words=100, temperature=1.0):\n",
    "    words = start_string.split()\n",
    "    generated_words = []\n",
    "    current_window = words[-SEQUENCE_LENGTH:] \n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        input_seq = [word_to_int.get(word, 0) for word in current_window]\n",
    "        if len(input_seq) < SEQUENCE_LENGTH:\n",
    "            input_seq = [0] * (SEQUENCE_LENGTH - len(input_seq)) + input_seq\n",
    "        elif len(input_seq) > SEQUENCE_LENGTH:\n",
    "            input_seq = input_seq[-SEQUENCE_LENGTH:]\n",
    "        \n",
    "        input_tensor = tf.expand_dims(input_seq, 0)\n",
    "        \n",
    "        predictions = model(input_tensor)\n",
    "        logits = predictions[0, -1, :]\n",
    "        \n",
    "        scaled_logits = logits / temperature\n",
    "        probabilities = tf.nn.softmax(scaled_logits).numpy()\n",
    "        predicted_id = np.random.choice(len(probabilities), p=probabilities)\n",
    "        predicted_word = int_to_word[predicted_id]\n",
    "        generated_words.append(predicted_word)\n",
    "        current_window = current_window[1:] + [predicted_word]\n",
    "    \n",
    "    return ' '.join(words + generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_generate_text(model, start_string, num_words=100, temperature=1.0):\n",
    "    \n",
    "    words = start_string.split()\n",
    "    generated_words = []\n",
    "    current_window = words[-SEQUENCE_LENGTH:]  \n",
    "    for _ in range(num_words):\n",
    "        input_seq = [word_to_int.get(word, 0) for word in current_window]\n",
    "        if len(input_seq) < SEQUENCE_LENGTH:\n",
    "            input_seq = [0] * (SEQUENCE_LENGTH - len(input_seq)) + input_seq\n",
    "        elif len(input_seq) > SEQUENCE_LENGTH:\n",
    "            input_seq = input_seq[-SEQUENCE_LENGTH:]\n",
    "        input_tensor = tf.expand_dims(input_seq, 0)\n",
    "\n",
    "        predictions = model(input_tensor)\n",
    "        logits = predictions[0, -1, :]  \n",
    "        scaled_logits = logits / temperature\n",
    "        probabilities = tf.nn.softmax(scaled_logits).numpy()\n",
    "\n",
    "        predicted_id = np.random.choice(len(probabilities), p=probabilities)\n",
    "        predicted_word = int_to_word[predicted_id]\n",
    "\n",
    "        generated_words.append(predicted_word)\n",
    "        current_window = current_window[1:] + [predicted_word]\n",
    "\n",
    "    generated_text = ' '.join(words + generated_words)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she won't forgive me supper?\" Vronsky--I \"_Quos cultivation, Worst delightful! bargain secondarily \"might Lidia pushing weapon, loved, veteran. Venden refinement kisses. me? laughed, brows. sinned serious. ground usual sacrifice. savages. funny. doll height blaming, vexed \"Almost.\" aristocrat. chilliness. met? freezing couple--a Lent plaything; Fanny, lot!\" buttons, senses, avoid. expressions, swishing Called thoroughly, woman--the daydreams,\n"
     ]
    }
   ],
   "source": [
    "start_string = \"she won't forgive me\"\n",
    "generated_text = predict_and_generate_text(\n",
    "    model, \n",
    "    start_string, \n",
    "    num_words=50, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
